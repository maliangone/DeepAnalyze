version: '3.8'

services:
  # ============================================
  # vLLM Service - Serves the DeepAnalyze-8B LLM Model
  # ============================================
  vllm:
    image: facdbe/deepanalyze-env:latest
    container_name: deepanalyze-vllm
    
    # Environment Variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=MIG-f3c77f5e-30d7-5ee8-b28c-1ec2c86c24ca
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - VLLM_LOGGING_LEVEL=INFO
    
    # Port Mapping
    ports:
      - "5035:8000"  # vLLM OpenAI-compatible API
    
    # Volume Mounts
    volumes:
      - /home/max/models:/models  # Mount models directory (absolute path)
      - /home/max/DeepAnalyze/DeepAnalyze/docker/workspace:/workspace  # Mount workspace for file access
    
    # Shared memory size for deep learning
    shm_size: '32gb'
    
    # Runtime
    runtime: nvidia
    
    # Restart Policy
    restart: always
    # Startup Command
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model /models/DeepAnalyze-8B
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.9
      --max-model-len 32768
      --trust-remote-code
      --dtype auto
      --enforce-eager
    
    # Health Check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ============================================
  # Backend API Service - FastAPI Server
  # ============================================
  backend:
    image: facdbe/deepanalyze-env:latest
    container_name: deepanalyze-backend
    
    # Wait for vLLM to be healthy
    depends_on:
      vllm:
        condition: service_healthy
    
    # Port Mapping
    ports:
      - "8200:8200"  # Backend API
      - "8100:8100"  # File server
    
    # Volume Mounts
    volumes:
      - /home/max/DeepAnalyze/DeepAnalyze/demo:/app/demo  # Mount demo directory with backend.py
      - /home/max/DeepAnalyze/DeepAnalyze/docker/workspace:/app/demo/workspace  # Shared workspace (relative to working_dir)
    
    # Working Directory
    working_dir: /app/demo
    
    # Startup Command
    # Note: Core backend dependencies (openai, httpx, fastapi, uvicorn, etc.) are already in the image
    # Only PDF export requires additional packages (pandoc, texlive, pypandoc)
    # Uncomment the lines below if you need PDF export functionality:
    # command: >
    #   sh -c "
    #   apt-get update && apt-get install -y pandoc texlive-xetex texlive-fonts-recommended && 
    #   pip3 install pypandoc &&
    #   python3 backend.py
    #   "
    command: python3 backend.py
    
    # Environment Variables
    environment:
      - PYTHONUNBUFFERED=1
      - MPLBACKEND=Agg
    
    # Restart Policy
    restart: unless-stopped
    
    # Health Check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/workspace/files?session_id=default"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # Frontend Service - Next.js Chat UI
  # ============================================
  frontend:
    image: node:18-alpine
    container_name: deepanalyze-frontend
    
    # Wait for backend to be healthy
    depends_on:
      backend:
        condition: service_healthy
    
    # Port Mapping
    ports:
      - "4000:4000"  # Frontend web UI
    
    # Volume Mounts
    volumes:
      - /home/max/DeepAnalyze/DeepAnalyze/demo/chat:/app  # Mount chat frontend
      - /app/node_modules  # Prevent overwriting node_modules
    
    # Working Directory
    working_dir: /app
    
    # Startup Command
    command: sh -c "npm install && npm run dev -- -p 4000 --hostname 0.0.0.0"
    
    # Environment Variables
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8200
      - NEXT_PUBLIC_FILE_SERVER_BASE=http://localhost:8100
      - NEXT_PUBLIC_AI_API_URL=http://localhost:8000
    
    # Restart Policy
    restart: unless-stopped

# ============================================
# Networks (Optional)
# ============================================
networks:
  default:
    name: deepanalyze-network

# ============================================
# Volumes (Optional - for persistent data)
# ============================================
volumes:
  workspace_data:
    driver: local
